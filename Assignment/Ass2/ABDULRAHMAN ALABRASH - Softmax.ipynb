{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying MNIST digits with a softmax classifier\n",
    "#Name: Abdulrahman Alabrash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a softmax classifier to predict the digit presented in a given image. We will use the MNIST dataset for this task. Please first skim through the notebook. Then complete the following steps mentioned in the main function:\n",
    "\n",
    "1. minibatch\n",
    "2. init_params\n",
    "3. forward and backward propagation\n",
    "    * softmax_forw\n",
    "    * softmax_cost\n",
    "4. grad_check\n",
    "5. train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; for p in [\"Knet\"]; haskey(Pkg.installed(),p) || Pkg.add(p); end #Knet installation to use the MNIST dataset\n",
    "using Knet, Printf, Random\n",
    "using Statistics\n",
    "include(Knet.dir(\"data\", \"mnist.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch2 (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " function minibatch2(X, Y, bs=100)\n",
    "    #takes raw input (X) and gold labels (Y)\n",
    "    #returns list of minibatches (x, y)\n",
    "    data = Any[]\n",
    "    \n",
    "    #start of step 1\n",
    "    #YOUR CODE HERE\n",
    "    for i in 1:bs:size(X,2)\n",
    "        push!(data,(X[:,i:i+bs-1],Y[:,i:i+bs-1]))\n",
    "    end\n",
    "    #end of step 1\n",
    "    return data                                                                 \n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: dtst not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: dtst not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[4]:4"
     ]
    }
   ],
   "source": [
    "xtrn = randn(784, 60000)\n",
    "ytrn = randn(10, 60000)\n",
    "minibatch2(xtrn,ytrn,100)\n",
    "summary.(dtst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_params (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_params(ninputs, noutputs)\n",
    "    #takes number of inputs and number of outputs(number of classes)\n",
    "    #returns randomly generated W and b(must be zeros vector) params of softmax\n",
    "    #start of step 2\n",
    "    #YOUR CODE HERE\n",
    "    w = randn(noutputs,ninputs).*0.001\n",
    "    b=zeros(noutputs,1)\n",
    "    return w,b\n",
    "    #end of step 2                                                              \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_forw (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax_forw(W, b, data)\n",
    "    #applies the affine transformation and softmax function\n",
    "    #returns predicted probabilities\n",
    "    \n",
    "    ### step 3_1\n",
    "    #YOUR CODE HERE\n",
    "    probs = W * data.+b\n",
    "    probs=exp.(probs)\n",
    "    probs ./= sum(probs,dims=1)\n",
    "    return probs\n",
    "    ### step 3_1                                                                  \n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax_cost(W, b, data, labels)\n",
    "    #takes W, b paremeters, data and correct labels\n",
    "    #calculates the soft loss, gradient of w and gradient of b\n",
    "    \n",
    "    #start of step 3_2\n",
    "    #YOUR CODE HERE\n",
    "    probs=softmax_forw(W,b,data)\n",
    "    @show size(nll)\n",
    "    @show size(probs)\n",
    "    cost=-sum(labels.*(log.(probs)))/size(data,2)\n",
    "    nll=-(labels-probs)/size(data,2)\n",
    "    ∇wb=sum(nll,dims=2)\n",
    "    ∇w=nll*data'\n",
    "    \n",
    "     return cost,∇w,∇wb\n",
    "    #end of step 3_2                                                          \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_check (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grad_check(W, b, data, labels)\n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.0001\n",
    "        \n",
    "        gw = zeros(size(W))\n",
    "        gb = zeros(size(b))\n",
    "        \n",
    "        #start of step 4\n",
    "        #YOUR CODE HERE\n",
    "        w_t = copy(W)\n",
    "        b_t = copy(b)\n",
    "        \n",
    "        for i = 1:size(b,1)\n",
    "            b0 = deepcopy(b) ; b1 = deepcopy(b)\n",
    "            b0[i] = b0[i] - epsilon ; b1[i] = b1[i] + epsilon\n",
    "            cost0,_,_ = softmax_cost(W, b0, data, labels)\n",
    "            cost1,_,_ = softmax_cost(W, b1, data, labels)\n",
    "            gb[i] = (cost1 - cost0) / (2*epsilon)\n",
    "        end\n",
    "        \n",
    "        for i = 1:length(W)\n",
    "            w0 = deepcopy(W) ;w1 = deepcopy(W)\n",
    "            w0[i] = w0[i] - epsilon ; w1[i] = w1[i] + epsilon\n",
    "            cost0,_,_ = softmax_cost(w0, b, data, labels)\n",
    "            cost1,_,_ = softmax_cost(w1, b, data, labels)\n",
    "            gw[i] = (cost1 - cost0) / (2*epsilon)                       \n",
    "        end    \n",
    "        \n",
    "        #end of step 4\n",
    "        \n",
    "        return gw, gb\n",
    "    end\n",
    "    \n",
    "    _,gradW,gradB = softmax_cost(W, b, data, labels)\n",
    "    gw, gb = numeric_gradient()\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "    end  \n",
    "                                                                            \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        #start of step 5\n",
    "        #YOUR CODE HERE\n",
    "        nll,∇w,∇b=softmax_cost(W,b,x,y)\n",
    "        #println(\"∇b---->\",nll)\n",
    "        numins+=size(x,2)\n",
    "        totalcost += nll*size(x,2)\n",
    "        \n",
    "        W .-= lr * ∇w\n",
    "        b .-= lr * ∇b \n",
    "    \n",
    "        #end of step 5\n",
    "    end\n",
    "    \n",
    "    avgcost = totalcost / numins                                                \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy1 (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function accuracy1(ygold, yhat)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += findmax(ygold[:,i]; dims=1)[2] == findmax(yhat[:, i]; dims=1)[2] ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)                                             \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "    Random.seed!(12345)\n",
    "    \n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "    \n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "    \n",
    "    ## Data loading & preprocessing\n",
    "    #\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,1,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "    #  Size of ytrn must be: (10, 10000)\n",
    "    #  Size of ytst must be: (10, 10000)\n",
    "    \n",
    "    xtrn, ytrn, xtst, ytst = mnist() # loading the data\n",
    "    xtrn = reshape(xtrn, 784, 60000)\n",
    "    xtst = reshape(xtst, 784, 10000)\n",
    "    \n",
    "    function to_onehot(x)\n",
    "        onehot = zeros(10, 1)\n",
    "        onehot[x, 1] = 1.0\n",
    "        return onehot\n",
    "    end\n",
    "    \n",
    "    ytrn = hcat(map(to_onehot, ytrn)...)\n",
    "    ytst = hcat(map(to_onehot, ytst)...)\n",
    "    \n",
    "    ## STEP 1: Create minibatches\n",
    "    # Complete the minibatch function\n",
    "    # It takes the input matrix (X) and gold labels (Y)\n",
    "    # returns list of tuples contain minibatched input and labels (x, y)\n",
    "    bs = 100\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "    \n",
    "    ## STEP 2: Initialize parameters\n",
    "    #  Complete init_params function\n",
    "    #  It takes number of inputs and number of outputs(number of classes)\n",
    "    #  It returns randomly generated W matrix and bias vector\n",
    "    #  Sample from N(0, 0.001)\n",
    "    \n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "    \n",
    "    ## STEP 3: Implement softmax_forw and softmax_cost\n",
    "    #  softmax_forw function takes W, b, and data\n",
    "    #  calculates predicted probabilities\n",
    "    #\n",
    "    #  softmax_cost function obtains probabilites by calling softmax_forw\n",
    "    #  then calculates soft loss and\n",
    "    #  gradient of W and gradient of b\n",
    "    \n",
    "    ## STEP 4: Gradient checking\n",
    "    #  Skip this part for the lab session.\n",
    "    #  As with any learning algorithm, you should always check that your\n",
    "    #  gradients are correct before learning the parameters.\n",
    "    \n",
    "    debug = true #Turn this parameter off, after gradient checking passed\n",
    "    if debug\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "    \n",
    "    lr = 0.15\n",
    "    \n",
    "    ## STEP 5: Training\n",
    "    #  The train function takes model parameters and the data\n",
    "    #  Trains the model over minibatches\n",
    "    #  For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "    #  train function returns the average cost per instance\n",
    "    \n",
    "    for i=1:50\n",
    "        cost = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy1(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy1(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 1.8295542693851845e-9\n",
      "Gradient Checking Passed\n",
      "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
      "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
      "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
      "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
      "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
      "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
      "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
      "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
      "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
      "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
      "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
      "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
      "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
      "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
      "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
      "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
      "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
      "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
      "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
      "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
      "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
      "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
      "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
      "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
      "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
      "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
      "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
      "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
      "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
      "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
      "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
      "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
      "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
      "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
      "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
      "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
      "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
      "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
      "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
      "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
      "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
      "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
      "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
      "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
      "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
      "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
      "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
      "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
      "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
      "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#= Example Output\n",
    "Diff: 1.8292339049184216e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "=#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
